{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Optimize Spark","text":"<p>To skip the complicated steps to start a Spark session, we build a convenient tool to automatically set up all the configurations and provide the optimal resource we need.</p>"},{"location":"index.html#spark-configuration","title":"Spark Configuration","text":""},{"location":"index.html#spark-configuration_1","title":"Spark Configuration","text":"<p>This section briefly introduces the basic theories of the spark configuration.</p>"},{"location":"index.html#spark-configuration-details","title":"Spark Configuration Details","text":"<p>This section explains the algorithm details on Optimize Spark.</p>"},{"location":"index.html#spark-configuration-documentation","title":"Spark Configuration Documentation","text":"<p>This section details the OptimalSpark class documentation </p>"},{"location":"index.html#resources","title":"Resources","text":"<p>Additional resource for spark</p>"},{"location":"resources.html","title":"PySpark Resources","text":"<ul> <li>Learning Spark - Lightning-Fast Data Analytics, 2<sup>nd</sup> Edition, JS Damji, B Wenig, T. Das and D.lee</li> <li>Tuning Apache Spark for Large Scale Workloads</li> <li>Hive Bucketing in Apache Spark</li> <li>Why You Should Care About Data Layout in the File System</li> <li>Decoding Memory in Spark</li> </ul>"},{"location":"spark_config.html","title":"Spark Configuration","text":"<p>Spark Configuration is essential to Spark performance, if properties are set properly ,  then they can boost up the Spark performnce. However, for inexperinced users, it is very  hard to set up Spark correctly.</p> <p>This tool provides a convient way to boost up the existing program, so users do not  need to worry about specific settings.</p> <p>For Spark resource allocation, the main two settings are total core_number and memory  size for each executor, while number of cores for each executor is also importent for  larger jobs.</p> <ul> <li> <p>Total Core numbers</p> <p>When the total core numbers matches the total partition number, the execution of   Spark's tasks achive exceedingly parallel. if the core number is limited, we need to   design the core numberto make the partition number be iyts integer multiples. This will   help maximum the paralleism.</p> </li> <li> <p>Executor Memory </p> <p>For all the cores on the same executor, they share the same memory. The memory's layout shows as below.</p> <p></p> <p>According to  Decording Memory in Spark,  the spark runtime segregates the memory in the driver and executor into  4 different parts:</p> <ol> <li>Storage Memory - memory reserved for cached data </li> <li>Execution Memory - memory used by data-structures during shuffle operations       (joins, group-by's and          aggregations).</li> <li>User Memory - For storing the data-structures created and manged by the           user's code </li> <li>Reserved Memory - Reserved by Spark for internal purpose</li> </ol> <p>The default division for execution memory and storage memory is 60\\% of   the total memory, after allowing for     300 MB for reserved memory, to safeguard against Out-of-Memory (OOM) errors.    The reamining User Memory is for used defined functions such as UDF</p> <p>When storage memory is not being used , Spark can acquire it for use in execution memory for execution  purposes, and vice versa.</p> <p>Execution memory is used for Spark shuffles, joins, sorts, and aggregations, Since different  queries may require different amounts of memory, the fraction (spark.memory.fraction is 0.6 by default) of the available memory to  dedicate to this can be  tricky to tune but it's easy to adjust. By contrast, storage memory is  primarily used for  caching user data structures and partitions dervied from Dataframes.</p> </li> </ul> <p>To optimize resource utilization  and maximize parallelism, the ideal allocation is at least as many partitions as there are cores on  the executor, as depicted in next figure in Learning Spark , 2<sup>nd</sup> Edition . if there are more partitions then there are  cores on each executor, all the cores are kept busy. you can think of partotions as  atomic units of parallelism: a single thread running on a single cre can work on a  single partition. </p> <p>To improve performance, some more settings are configured based on recommendations  from multiple sources (see Resources).</p>"},{"location":"spark_config.html#know-your-data-and-task","title":"Know Your Data and Task","text":"<p>For Spark tasks, it is very importent to know your data before requesting resources. If you have a small dataset, it would be overprovisioning (e.g., allocated but unused cores), and cause unnecessary costs; wwhile it would be underprovisioning(e.g. , not sufficient cores for partitions) for large  scale data may lead to task failure .</p> <p>In addition, if the Spark tasks involves exploding data (i.e., expanding data  exponentially), the originally allocated resources may not be enough to complete  required job, which may lead to OOM failure . in this case, it is very important to plan the resources wisely. Howver, for inexpericed  users, it is very difficult to decide how many cores or memory to set aside in advance.</p> <ul> <li> <p>Work Load</p> <p>For the workaround, we can expect the work load for the tasks. The work  load can help us get a better estimate on the required resources. Below  is a list of work load for selected projects </p> Projects Exploding Data Work Load Examples Shuffle Percentage Data Analysis No Light (L) Basic Statistics Summaries &lt;20\\% Data operation No Light (L) Data modification, add/remove columns, etc. &lt;10\\% Model Evalaution No Medium (M) Data Aggregation, Combining , etc.. 30\\% Model Fitting No Medium Heavy (MH) Data classifier, Tree building, etc. 50\\% Model Exploration No Heavy (H) Multiplier iterations of Model Fitting &gt;50\\% Model Forcast No Extremely Heavy (XH) Exploding data, multiple operations on data joining and aggregation, etc. &gt;80\\% <p>The shuffle operation includes joins, sorts, and aggregations. When the partitions  need to be \"re-arrayed\" to get the output, Spark needs to shuffle the data, so partitions from cores need to be reassigned.Such operations consume the most  time and resources </p> </li> <li> <p>File Size</p> </li> </ul> <p>In the HWX HDFS, the default replication factor is 3. So when we use du to     check the file size, it usually lists three columns     <pre><code>(pyfarmbase) hadoop fs -du -h -s [file/directory to be loded]\n[size in MB] [disk space consumed] [file/dir name]\n</code></pre></p> <p>Before starting Spark, we need to run the above command to get an estimate of the     file storage size </p> <p>On the other hand, when Spark writes the files to disk, it compresses the data     with specific codec, such as snappy ,lz4, gzip, etc. These compression codec    usually provide good compression ratios. The larger data, the higher the     compression ratio is.It is hard to estimate the actual data size by looking solely     at the file size without loading the data into Spark, but we expect the real     data size is at least 4-5 times larger then actual file size.</p> <p>If you expect data will be exploded to a certain rate uin spark operation    (e.g, snapshot data may be exploded to over 100 times if the forecast     horizon is over 100 months ), we define this rate as exploding factor r.    If there is no exploding operation for the data , the default exploding factor is 1.</p> <p>We define the file storage size as S in GB, and make the following assumptions:    The real data size is z times larger then the actual file size. In practice       z can be 4, 5, or larger. When the file size less then 1G, we set z=5. Otherewise,      we set it to 4, or best estimate on experience.</p>"},{"location":"spark_config.html#spark-configuration-tool","title":"Spark Configuration Tool","text":"<p>we design a configuration tool for development purpose. By using this tool, one does not need to specify each setting individually any more . This tool  will provide a convenient way to allocate sufficient and optimal resource when the developers provides the file size and work load for their work.</p> <p>The following code will obtain a spark session for a modelling job that have an input file with 1GB and moderate work load. <pre><code>from tools.util.optimize_spark import * \n\nspark = OptimizeSpark(app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\")\\\n                 .start_optimal_spark(file_size=1, job_load=\"m\")\n</code></pre> It also provides an estimate of the total resources requested  <pre><code>Driver Settings: spark.driver.cores: 7 \nDriver Settings: spark.driver.memory: 13g \nExecutor Settings: spark.executor.memory: 3g\nExecutor Settings: spark.executor.cores: 2\nExecutor Settings: spark.executor.instances: 52\nRequested Total Estimate: 163 Core and 187.78 GB Memory \n</code></pre></p> <p>For details of how the configuration settings are calculated, please go to Configuration Details</p>"},{"location":"spark_config_details.html","title":"Spark Configurations Details","text":"<p>Based on Basic Spark Configuration Guideline, we design a configuration  tool for development purpose.</p> <p>By using this tool, one does not need to specify each setting individually any more. The tool will provide a convenient way to allocate sufficient and optimal resource when the developer  provide the job size and load.</p>"},{"location":"spark_config_details.html#local-mode","title":"Local Mode","text":"<p>For local mode, number of executor is set to be 1, as all the cores are on the same computer. The total core/memory used in Spark takes one third of the       available cores/memory on the node. to ensure smooth process for       other usess.</p> <p>For heavy load that involves over 60\\% of tasks in      shuffling, joining, and aggregating, etc., one      half of the resources will be used.     However, if it leads to insufficient core of memory,     the program will be terminated </p> <p>for local core, with 1 left for application master , we set driver  has 1/3 of the available core, and leave the  reamining cores to executors.</p> <p>For local memory, We need to get the total memory for each executor and drive before  setting up the Spark memory. We also need to make sure to  reserve 10\\% for memoryOverhead.</p> <p>In particular., we define total available core for the local node to be N, partition_size as M_p, and total memory to be M, then we design the resources as below.</p> Resource Design executor number (N_e) 1 core per executor (C_e) \\min(zS/M_p, [N/3^2])\\times [(F+1)/2] driver core number (C_d) \\max(1, \\min([N/3] - C_e\\times N_e - 1, F+1)) driver memory (M_d) ceil(\\min(zS\\times [\\frac{F+1}{2}], M/3^2\\times 0.9)) executor memory (M_e) ceil(\\max(1, \\min(\\min(r,5)\\times \\max(M_p, [\\frac{zS}{C_e}])\\times C_e[\\frac{F+1}{2}]/0.4,(M/3 - M_d/0.9)\\times 0.9))) <p>[X] is the integer part for X.</p>"},{"location":"spark_config_details.html#yarn-mode","title":"Yarn Mode","text":"<p>For yarn mode, the design is different then local mode , scince the cluster is shared with all team members,     and cannot be solely used by one user, so an optimal design is much better then taking all      the resources up. The design is based on file size and file load </p> <p>In a typical yarn cluster, it usally provides sufficient memory and cores to use. For a spark session, it is good practice to request optimal memory and core numbers , instead of  taking up as much as one can.</p> <p>For example, we consider a Spark task on a small data( e.g, with 256MB size) and the most complicated calculation for this data is summerizing statistics. if we request 100 cores  in 50 executors, and 2GB for each executor, we will take </p> <ul> <li> <p>a total of 150 cores: since each executor reservers 1 core for application master,       the total core number is 100 + 50 = 150</p> </li> <li> <p>125GB memory: the total required memory includes the reserved 300MB memory.In addition         each executor will needs to reserve 10\\% of the executor memory for memory overHead,        so the total memory used is ((1+0.1)\\times2GB+300MB)\\times 50 =125GB.</p> <p>For such small data and simple calculate, it not only wastes a lot of resources not arhieving    best performance, but also may result in whats's known  as the \"small file problem\" - many small partition files, introducing an inoridinate  amount of disk I/O and performance degradation thanks to filesystem operations such as opening, closing, and listing directories, which on a distributed filsystem can be slow.</p> </li> </ul> <p>We do not have a magic schema to design the best allocation for all types of jobs. However, based on experience and multiple resources , we design an algorithm to try to use best of the resources while achieving high performance.</p> <ul> <li> <p>Available Resources</p> <p>In HWX server, there are a total of 155 nodes (i.e executors), 70.39TB memory  and 11,780 cores. For the Card usage, We have an effective capacity  of 1413 cores and 8.45TB (i.e., 8649GB)</p> <p>As the clusters are shared among all team members, we need to use the resources  wisely, and try to request the resources within capacity with best performance.</p> <p>Based on these numbers, we recommend that the maximum cores for ine application should be within 500, and maximum memory 29GB, which is one thrid of the queue allowance.</p> <p>Although each cluster nodes has 78 cores, and 56GB memory in average, to realize best perfomnce, we limit the core number for each executor to 5, as recommended by the MIT team. And to make sure other uses can run their applications successfully, we further limit the executor memory to 25GB. Memory used more then that  may cause OOM issues for all the applications running on the clusters.</p> </li> </ul>"},{"location":"spark_config_details.html#driver-resources","title":"Driver Resources","text":"<p>We need to set up sufficient cores and memory for the driver nodes. At a high level on spark Architecture, a Spark  apllication consists of a driver program that is responsible for orchestrating parallel  operations on the spark cluster. The driver acces is distributed components in  the cluster-the Spark executors and cluster manager-through a SparkSession. See the figure below in Learning Spark , 2<sup>nd</sup> Edition .</p> <p></p> <p>The  Spark driver has multiple roles: it communicates with the cluster manager; it requests  resources (CPU, memory, etc) from the cluster manager for Spark's executors (JVMs); and it transforms all the Spark operations into DAG computations, schedules  them, and distributes their execution as tasks across the Spark executors.Once the  resources are allocated, it communicates directly with the executors.</p> <p>Therefore , it is very important the driver takes abundant resources. We define  the job load factors based on the work load defined above:</p> Job Load Factor F Work Load 1 Light(L) 2 Medium(M) 3 Heavy(H) 4 Medium Heavy (MH) 5 Extremely Heavy (XH) <p>To achieve best perfomance, we allocate at least 2 cores to the driver, even for small jobs. However, the total core number should be within one third as of total core number in one cluster node (i.e., 25 cores), which will be served as the driver. We then design the driver core number C_d as:</p> <p>$$ C_d = \\min(25, \\max(2, ceil(zS\\times f_0/2))), $$ where f_0=2\\log_2^{F+1}; and driver memory M_d in GB as:</p>  M_d = \\min(40, zS\\times f_0)  <p>in which we set the driver memory at least double of the data size , multipled by the job load factor. Even if the data is exploded, the main operation is in clusters  instead of  the driver, so we do not need to assign the driver huge memory. We set a limit  of 40GB to reserve sufficient memory use.</p> <p>ceil(x, t) means the number x will be rounded to nearest t. We round the memory by 8GB with common practice, and round core number  by 4 so that it can be better allocated to executors.</p>"},{"location":"spark_config_details.html#total-core-number-for-executors","title":"Total Core Number for Executors","text":"<p>When a data is loaded in Spark, Spark recognizes its partition numbers, and  allocates each partition with one core, if the cores are sufficient.If the  core number is greater than the partition number, the remaining cores may  not participate in Spark Jobs, Hence wsate the resources; if the core number  is smaller than the partition number, it will need to take more time to work  on remaining partitions.</p> <p>By default, the size of a partition in spark is 128MB, Smaller partition size may  lead more I/O operations and communications among cores accross multiple  executors (i.e cluster nodes), hence decrese  the work performance;while larger partition size may slow down the  calculation spped by individual cores. Therefore, balancing between partition number and size is extreamly important for Spark resource allocation.</p> <p>We make the following two assumptions :</p> <ul> <li>The default partition size is 128MB</li> <li>The maximal core we request for executors is 750.</li> </ul> <p>We design the initial total core number C_1 as  $$ C_1 = \\min(750, z\\times ceil(S) \\times f_0 \\times 2^3). $$</p>"},{"location":"spark_config_details.html#memory-allocation","title":"Memory Allocation","text":"<p>To design the executor memory, we need to firstly decide the number  of cores for each executor. This is consided based on job load  and job size.</p> <p>By deafult, each partition is handled by one core, and each partition takes 128MB. From the memory layout, if an executor has only one core to work for one partition, We need to assign  (128/0.4 + 300) = 620MB. For small josb, this may be sufficient; but for larger or heavier jobs, we suggest at least twice larger memory for heavy suffle operations</p> <p>More cores to the same executor will reduces communication time, and  it will help the executors locate closest partitions as many as possible.' However, due to the memory and core limit, We need to balance betwwen them.</p> <p>We define the partition size as M_p MB, and design the core number per executor C_e as:</p> <p>$$ C_e = \\min(F, ceil(\\frac{25\\times 2^{10}}{F\\times M_p /0.4 + 300})). $$ The maximum C_e is \\max(F)=5, which is consitent with the MIP teams's recemmedation.</p> <p>For the total memory on each executor, defined as M_e in GB, we design </p> <p>$$ M_e = \\min(25, ceil(C_e \\times \\frac{\\max(M_p \\times f_0, zSr/C_1)}{0.4} + 330\\times 2^{-10}, 1)) $$ In this defination, zSr/C_1 estimates the memory each core will use for data storage, and M_p\\times F estimates the memory for complicated partition calculations</p>"},{"location":"spark_config_details.html#executor-number","title":"Executor Number","text":"<p>Finally, we define the executor number N_e, as this is the configuratio parameter set in the Spark Session. Based on the  total core number C_1 and  core number per executor C_e, and that there is 155 total executors, we have:</p>  N_e = ceil(C_1 / C_e, C_e)  <p>With this defination, the total core C_2=N_e\\times C_e we request for executors may be slightly more than C_1, since the cores need to be evenly distributed  across the executors. Therefore, we expect serveral spare cores by this design.</p>"},{"location":"spark_config_details.html#additional-adjustment","title":"Additional Adjustment","text":"<p>As HWX uses Yarn to manage all the resources, the actual resource allocation scheme may be not exactly as the same as theories listed above. By experience, we make some  ad-hoc adjustment to fit better to the HWX servers, so better performance can be reached.</p>"},{"location":"spark_config_details.html#total-requested-resources","title":"Total Requested Resources","text":"<p>Each executor will reserve one extra core for application master, so  the total core we request for the entire Spark session is </p>  \\text{total_core} = C_d + (\\tilde{C}_e + 1)\\times N_e.  <p>The total memory calaculation is more complicated. In addition to the reserved  memory on each executor, the executor also reserve 10\\% of the memory for  memory buffer, which is callwd \"memory OverHead\".</p> <p>Nevertheless, it is very likely that    the cluster administrators have set some additional memory settings, so that     the real memory allocation is different from what we have calculated bases on     knowledge from the books.</p> <p>We estimate the total memory by    including the driver memory as </p>  \\text(total_memory) = M_d / 0.9 + M_e / 0.9 \\times N_e.  <p>However, Please note that this only an estimate. Please consult with  the cluster administer for actual resource allocation algorithm.</p>"},{"location":"spark_config_details.html#large-memory-use-with-data-exploration","title":"Large Memory Use with Data Exploration","text":"<p>In model forecast for CCAR and CECL, it is a common practice to explode data, which  expands the data exponentially. In this situation, large memory needs to be reserved  to support the program running succesfully .</p> <p>To allocation enough resource while not talking too much, one needs a lot of experince. Our tool provides a function that will request sufficient resource to run the program with  no memory errors. However, the provided resources may not be optimal, as different forecast engine have different requirment, and the tool cannot provide a FTT-FOR-ALL approach.</p> <p>Nevertheless , one may use the tool as a strting point and tune the resources to optimal  based on experince.</p>"},{"location":"utils.html","title":"Spark Configuration Documents","text":"<p>This module optimizes the spark setting based on resource allocation algorithm defined on </p> <p>The module defines the following class </p> <ul> <li>OptimizeSpark<ul> <li>It configures settings to optimize the Spark performance based on the provided job size and load </li> <li>It also defines functions to optimize performance, such as repartition, and boost up perfomance </li> </ul> </li> </ul> <p>Author: Bibhudutt Mohapatro</p>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark","title":"<code>OptimizeSpark</code>","text":"<p>A configuration class that optimize the performance of Spark session</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>class OptimizeSpark:\n    \"\"\"\n    A configuration class that optimize the performance of Spark session \n    \"\"\"\n\n    def __init__(self, app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\", \n                       reset_port=False, log_yaml=\"configs/logging.yml\",\n                       config_properties=None, timezone='UTC', **kwargs) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            app_name (str, optional): The application name that identfies the spark job\n            spark_queue (str, optional): the queue that is assigned to the user, it is required parameter and must be specified .\n            spark_master (str, optional): yarn for cluster, and local for client, in which only the edge node isused\n            reset_port (bool, optional): reset port to 0 if True, due to Jupyter hub restriction \n            log_yaml (str, optional): log yaml configuration file for logging. By defaut, it uses an undefined setting.\n            config_properties (dict, optional): additional configurations used for spark.conf. when \n                config_properties={\"spark.sql.autoBroadcastJoinThreshold\":-1},\n                the configuration property spark.sql.autoBroadcastJoinThreshold\n                is set to be -1\n            timezone (str, optional): the timezone for the spark session, UTC, EST, PST, etc \n            **kwargs (dict): additional parameters for optimize resources , inclusing file_size, job_load,\n                 exploding_factor,zip_ratio etc.\n\n        Examples:\n            &gt;&gt;&gt;&gt; # loading the OptimizeSpark class from the module optimize_spark \n            &gt;&gt;&gt;&gt; from op_spark import * \n            &gt;&gt;&gt;&gt; # apply the logging setting defined in logging.yml\n            &gt;&gt;&gt;&gt; root_path =\"/home/CODE_DIR\"\n            &gt;&gt;&gt;&gt; opts = OptimizeSpark(app_name=\"optimize spark\", spark_queue=\"default\",\\\n                          spark_master=\"yarn\", log_yaml=os.path.join(root_path,\"configs/logging.yml\")\n            &gt;&gt;&gt;&gt; opts \n            &lt;op_spark.optimize_spark.OptimizeSpark at 087hyt677&gt;\n        \"\"\"\n\n        # loading logging info \n\n        if config_properties is None:\n            config_properties = {}\n\n        # get logging info \n        self.logger = set_logger_format(config_file=log_yaml, classname=__name__)\n\n        self.conf = SparkConf().setAppName(app_name)\n        # initialize resourse settings\n        self.__init_resource()\n\n        # Switch on yarn or local \n\n        if spark_master.lower() not in [\"yarn\", \"local\"]:\n            self.logger.error(\"The input spark master %s is invalid. \"\n                              \"it must be one of the following: Local or Yarn.\\n\"\n                              \"No Spark Session is initialized.\", spark_master)\n\n            return\n        elif spark_master.lower() != \"yarn\":\n            # parallelism selection for local mode \n            self.set_spark_master(\"local\")\n            self.logger.info(\"Local Mode\")\n\n        else:\n            self.set_spark_master(spark_master)\n            self.logger.info(\"Yarn Mode\")\n\n        self.exploding_factor = 1\n        self.default_setting = True \n        self.conf.setMaster(self.spark_master)\n        self.conf.set(\"spark.yarn.queue\", spark_queue)\n        self.__set_conf(config_properties, reset_port, timezone)\n\n\n    def start_spark(self, resource: dict =None):\n        \"\"\"\n        Start a Spark Session with provided resource.\n\n        Args:\n            resource (dict): spark resources to start spark session. It lists \n            [driver_core_num, driver_memory, executor_num, executor_memory, core_per_executor]. if not provided \n            start_spark will use the default spark settings.\n\n        Returns:\n            SparkSession: the spark Session to be used for spark applications\n\n        Examples:\n            &gt;&gt;&gt;&gt; my_resource = { 'driver_core_num': 1, 'driver_memory:'1g', 'executor_num': 3,\n                                 'executor_memory': '2g', 'core_per_executor': 3}\\\n            &gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\").start_spark(my_resource)\n\n        \"\"\"\n\n        if resource is None :\n            resource = {}\n        if len(resource) &gt; 0:\n            self.__set_resource(resource)\n\n        if not self.default_setting:\n            self.logger.info(\"Initializing OptimizeSpark with Customized Resources\")\n            self.__set_resource_conf()\n\n        else:\n            self.logger.info(\"Initializing OptimizeSpark with Default Resources\")\n\n        self.spark = SparkSession.builder.config(conf=self.conf).enableHiveSupport().getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n        self.get_spark_info()\n        self.get_resource_info()\n        return self.spark\n\n\n    def start_optimal_spark(self, resource=None, **kwargs):\n        \"\"\"\n        Start a Spark Session with provided resource.\n\n        Args:\n            resource (dict): spark resources to start spark session. It lists \n                     [driver_core_num, driver_memory, executor_num, executor_memory, core_per_executor]. if not provided \n                     start_optimal_spark will calculate the optimal resources, apply boosting configurations,\n                     and start spark sessions.\n                     if provided, start_optimal_spark will apply the boosting configuraton, and provided resource to start \n                     spark session\n\n            **kwargs: parameters used in optimize_resources \n\n        Examples:\n            &gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize_spark\", spark_queue=\"default\",\n                           spark_master=\"yarn\").start_optimal_spark(file_size=1, job_load='m')\n        \"\"\"\n\n        if len(kwargs) &gt; 0:\n            self.optimize_resources(**kwargs)\n\n        if resource is None:\n            self.__boost_conf(self.driver_memory)\n            return self.start_spark()\n\n        else:\n            self.__boost_conf(resource[\"driver_memory\"])\n            return self.start_spark(resource=resource)\n\n\n\n    def get_spark_session(self):\n        \"\"\"\n         Get the Spark Session generated \n         Returns:\n             SparkSession: the spark session created by the proceeding operations on OptimizedSpark\n        \"\"\"\n        return self.spark\n\n    def __boost_conf(self, driver_memory=None):\n\n        \"\"\"\n           Boosting configurations when higher Spark Perfomanse is requested \n        \"\"\"\n        self.logger.info(\"Configuration Boosting for Large Scale Data\")\n\n        # failure handling \n        self.conf.set(\"spark.max.fetch.failures.per.stage\", 10) #default to 4\n\n        #rpc \n        self.conf.set(\"spark.rpc.io.serverThreads\", 64) # default 0 \n\n        # i/o\n        self.conf.set(\"spark.unsafe.sorter.spill.reader.buffer.size\", \"1m\") #default 32, recommend 1m \n        self.conf.set(\"spark.file.transferTo\", \"false\") #default true \n        self.conf.set(\"spark.io.compression.lz4.blockSize\", 512) #default 32\n\n        # shuffle \n\n        self.conf.set(\"spark.shuffle.registration.timeout\",\"2m\") #5000\n        self.conf.set(\"spark.shuffle.registration.maxAttempts\", 5 ) # default 3 \n\n        if driver_memory:\n            max_result_size = \"{:.0f}g\".format(ceil(max(1, float(driver_memory[:-1])/4)))\n            self.conf.set('spark.driver.maxResultSize', max_result_size)\n\n\n    def optimize_resources(self, file_size=3, job_load=\"M\", \n                           exploding_factor=1, zip_ratio=4, overheap_adjust=False, **kwargs):\n\n        \"\"\"\n        Calculate optimal resource based on know info \n\n        Args:\n            file_size (int): the estimated file size from its pysical location \n            job_load (str): the estimated work load for the whole spark session \n            zip_ratio(int): the estimated zip ratio of memory to file size \n            exploding_factor (int): if the input data will be transformed and expanded exponentially, this parameter \n                 helps to estimate how many times the data will be exploded \n            overheap_adjust (bool): whether overlap needs to be adjusted \n            **kwargs: additional parameters for optimal_local_resource or optimal_yarn_resource\n        \"\"\"\n\n        self.exploding_factor = exploding_factor\n        self.overheap_adjust = overheap_adjust\n        # redesign file size if provided by string \n        if type(file_size) == int:\n            f_size = file_size\n        else:\n            self.logger.warning(\"File Size is provided by a string %s, please verify.\", file_size)\n            f_num = float(re.findall(\"\\d+\", file_size)[0])\n            f_unit = re.findall(\"\\D\", file_size)\n            if len(f_unit) == 0:\n                f_size = ceil(f_num)\n            elif f_unit[0].lower() == 'm':\n                f_size = ceil(f_num/1024)\n            elif f_unit[0].lower() == 'g':\n                f_size = int(ceil(f_num))\n            else:\n                self.logger.error(\" Strings for the file size can only be parsed with unit 'm'/'g' for 'MB'/'GB'. \\n \"\n                                  \"Otherwise, please use intergers for file size in GB \")\n\n        self.logger.info(\"Initializing OptimizeSpark with Job Size %d GB, Job Load %s\", f_size, job_load)\n        self.logger.info(\"Estimated Real Data Size %d GB\", f_size * zip_ratio)\n        if self.spark_master.lower() != \"yarn\":\n            # parallelism selection for local mode \n            if exploding_factor &gt; 5:\n                self.logger.warning(\"Local resource is not abundant for large datasets with exploding_factor &gt; 5.\"\n                                    \"Id the spark session fails, plaese consider Yarn mode. \\n\")\n            self.optimize_local_resources(file_size=file_size, job_load=job_load, zip_ratio=zip_ratio)\n            self.set_spark_master(\"local\" + \"[\" + str(self.driver_core_num) + \"]\")\n        else:\n            self.optimize_yarn_resources(file_size=file_size, job_load=job_load, zip_ratio=zip_ratio, **kwargs)\n\n\n    def set_spark_master(self, spark_master):\n        \"\"\"\n        Set spark master to 'local' or 'yarn'\n\n        Args:\n            spark_master (str): \"local\" or \"yarn\" mode.\n        \"\"\"\n\n        self.spark_master = spark_master\n\n    def __set_resource(self, resource):\n        \"\"\"\n        Set up resources to the Spark Session\n\n        Args:\n            resource (dict): spark resources to start spark session, it lists\n                 [driver_core_num, driver_memory, executor_num, executor_memory,core_per_executor].\n        \"\"\"\n        self.default_setting = False \n        self.driver_memory = resource['driver_memory']\n        self.executor_num = resource[\"executor_num\"]\n        self.driver_core_num = resource['driver_core_num']\n        self.core_per_executor = resource['core_per_executor']\n        self.executor_memory = resource['executor_memory']\n\n        # set up total core for perfomance tuning \n\n        self.total_exe_core = self.executor_num* self.core_per_executor\n\n        # set up shuffle partition for joining and aggregation \n\n        self.conf.set(\"spark.sql.shuffle.partitions\", self.total_exe_core) # default 200 \n\n    def get_spark_info(self):\n        \"\"\"\n        Provide an overview of the spark useful links used for development or moniter purpose \n        \"\"\"\n\n        sc = self.spark.sparkContext \n\n        # get UI address \n        uiUrl = \"https://{hostname}:12443/gateway/default/yarn/proxy/\" + sc.applicationId\n        self.logger.info(\"UI address: \\n %s \\n\", uiUrl)\n\n        # get scheduler address \n\n        scdlrUrl = \"https://{hostname}:61443/gateway/default/yarnuiv2/redirect#/yarn-app\" + sc.applicationId\n\n        self.logger.info(\"Scheduler address: \\n %s \\n\", scdlrUrl)\n\n\n    def get_resource_info(self):\n        \"\"\"\n        Provide an estimate of the resouse settings\n        Returns:\n           dict: a dictionary of resources, i.e., {\"spark.driver.cores\":1,\n                      \"spark.driver.memory\": \"1g\",\n                      \"spark.executor.memory\": \"1g\",\n                      \"spark.executor.cores\": 1,\n                      \"spark.executor.instances\": 2}\n        \"\"\"\n\n        driver_settings = {\"spark.driver.cores\": self.driver_core_num,\n                           \"spark.driver.memory\": self.driver_memory}\n\n        exe_settings = {\"spark.executor.memory\": self.executor_memory,\n                        \"spark.executor.cores\": self.core_per_executor,\n                        \"spark.executor.instances\": self.executor_num}\n\n        for res, setVal in driver_settings.items():\n            if setVal is None:\n                driver_settings[res] = self.defaultConf[res]\n                self.logger.warning(\"Driver Settings: %s not set, default is : %s\", res, self.defaultConf[res])\n            else:\n                self.logger.info(\"Driver settings: %s: %s\", res, setVal)\n        for res, setVal in exe_settings.items():\n            if setVal is None:\n                driver_settings[res] = self.defaultConf[res]\n                self.logger.warning(\"Executor Settings: %s not set, default is : %s\", res, self.defaultConf[res])\n            else:\n                self.logger.info(\"Executor settings: %s: %s\", res, setVal)\n\n        self.__estimate_resource(driver_settings, exe_settings)\n\n        return {\n                      \"spark.driver.cores\": self.driver_core_num,\n                      \"spark.driver.memory\": self.driver_memory,\n                      \"spark.executor.memory\": self.executor_memory,\n                      \"spark.executor.cores\": self.core_per_executor,\n                      \"spark.executor.instances\": self.executor_num\n        }\n\n\n    def __estimate_resource(self, driver_settings, exe_settings):\n        \"\"\"\n        Estimate resource total based on driver and executor settings\n\n        Args:\n            driver_settings (dict): driver core spark.driver.cores and memory spark.driver.memory \n            exe_settings (dict): executor information including spark.executor.cores,spark.executor.instances,\n            and spark.executor.memory \n        \"\"\"\n\n        # get total required core, each executor reserves 1 core for application master \n        total_core = driver_settings[\"spark.driver.cores\"] + (exe_settings[\"spark.executor.cores\"] + 1) * \\\n                     exe_settings[\"spark.executor.instances\"]\n\n        # get total required memory, which includes the memoryOverhead for both driver and executor \n\n        total_memory = int(driver_settings[\"spark.driver.memory\"][0:-1]) / (1-0.1) \\\n                       + (int(exe_settings[\"spark.executor.memory\"][0:-1]))* \\\n                       exe_settings[\"spark.executor.instances\"] / (1-0.1)\n        self.logger.info(\"Requested Total Estimate : {0:d} Core and {1:,.2f} GB Memory\"\n                         .format(total_core,total_memory))\n\n\n    def __set_conf(self, config_properties, reset_port, timezone):\n        \"\"\"\n        Set up all normal configurations for spark setup \n\n        Args:\n            config_properties (dict):  a dictionary of config properties to be set additionally \n            reset_port (bool): Setting for jupiterHub restriction\n        \"\"\"\n        utz = \"-Duster.timesone=\" + timezone.upper().strip()\n        self.conf.set(\"spark.sql.session.timeZone\", timezone.upper())\n        self.conf.set(\"spark.executor.extraJavaOptions\", utz)\n        self.conf.set(\"spark.driver.extraJavaOptions\", utz)\n        self.logger.info(\"Timezone set to be %s\", timezone.upper())\n\n        # set logging settings\n        self.conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n        self.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n        self.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n        self.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\n\n        # multiple contexts \n        self.conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n        self.conf.set(\"spark.storage.cleanupFilesAfterExecutorExit\", \"true\")\n        self.conf.set(\"spark.worker.cleanup.enabled\", \"true\")\n        self.conf.set(\"spark.worker.cleanup.interval\", 5)\n        self.conf.set(\"spark.worker.cleanup.appDataTtl\", 5)\n        self.conf.set(\"spark.python.use.daemon\", 'false')\n\n        self.conf.set('spark.sql.autoBroadcastJoinThreshold', '-1')\n\n        if len(config_properties) &gt; 0:\n            for confKey, confValue in config_properties.items():\n                self.conf.set(confKey, confValue)\n\n        # set up port \n\n        if reset_port:\n            self.conf.set(\"spark.ui.port\", 0)\n        import os \n\n        self.conf.set('spark.yarn.staingDir', os.environ[\"SPARK_YARN_SG_DIR\"])\n\n    def __init_resource(self):\n        \"\"\"\n        Initialize the resources and default values from spark\n        \"\"\"\n        self.driver_memory = None \n        self.driver_core_num = None \n        self.executor_num = None \n        self.core_per_executor = None \n        self.executor_memory = None \n        self.partition_size = 128/2 ** 10 \n\n        # default list for spark on these resources\n\n        self.defaultConf = {\n          \"spark.driver.cores\": 1,\n          \"spark.driver.memory\": \"1g\",\n          \"spark.executor.memory\": \"1g\",\n          \"spark.executor.cores\": 1,\n          \"spark.executor.instances\": 2\n        }\n\n\n    def __set_resource_conf(self):\n        \"\"\"\n        Based on the resource information , set up the configuration settings for reources \n        Returns:\n        \"\"\"\n\n        # driver settings\n\n        self.conf.set(\"spark.driver.memory\", self.driver_memory)\n        if self.spark_master.lower() == \"yarn\":\n            self.conf.set(\"spark.driver.cores\", self.driver_core_num)\n\n        # Executor settings\n\n        self.conf.set(\"spark.executor.instances\", self.executor_num)\n        self.conf.set(\"spark.executor.cores\", self.core_per_executor)\n        self.conf.set(\"spark.executor.memory\", self.executor_memory)\n\n        try:\n            varobj = getattr(self, \"exploding_factor\")\n            exploding_set = True \n\n        except AttributeError as error:\n            exploding_set = False \n            self.logger.warning(error)\n            self.logger.info(\"exploding_factor is taken as default 1.\")\n\n        if exploding_set:\n            if self.exploding_factor &gt; 10:\n                # offheap \n                self.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n                mem_num = float(re.findall('\\d+', self.executor_memory)[0])\n                mem_unit = re.findall(\"\\D\", self.executor_memory)\n\n                # overhead memory  should be at least 384\n\n                if len(mem_unit) == 0:\n                    memory_overhead = max(384, mem_num / 2 **10)/4 \n                elif mem_unit[0].lower() == 'm':\n                    memory_overhead = max(384, mem_num) / 4\n                else:\n                    memory_overhead = max(384, mem_num*2**10)/4\n\n                heap_size = \"{:.0f}m\".format(ceil(memory_overhead*0.6))\n                self.conf.set(\"spark.memory.offHeap.size\", heap_size)\n                # overhead = offHeap + 0.25* exe_num . for spark.3.0+, not need to include\n                # before spark 2, overhead needs to include offHeap\n                overhead = \"{:.0f}m\".format(memory_overhead)\n                self.conf.set(\"spark.executor.memoryOverhead\",overhead) # default is 10% of executor memory \n                self.logger.info(\"spark.executor.memoryOverhead is reset to %s\", overhead)\n                self.logger.info(\"spark.memory.offHeap.size is reset to %s \", heap_size)\n\n\n    def optimize_local_resource(self, file_size=3, job_load=\"M\", zip_ratio=4):\n        \"\"\"\n        Calculate optimal resource for Local mode, update the resource info based on the results \n\n        Args:\n            file_size (int, optional):  the estimated file size from its physical location.\n            job_load (str, optional): the estimated work load for the whole spark session.\n            zip_ratio (int, optional): the estimated zip ration of memory use to file size .\n        \"\"\"\n\n        real_size = zip_ratio * file_size \n\n        # get load factor \n        f = self.__get_job_factor(job_load)\n        # get total core on the node \n        n_core = int(subprocess.getoutput(\"nproc\"))\n        self.logger.debug(\"total core : %d\", n_core)\n\n        # get total memory of the node as 1/3 of total free memory, with MB as unit\n        total_node_mem = float(re.compile('\\D').sub('', subprocess.getoutput(\"grep MemFree /proc/meminfo\"))) / (1024**2)\n        self.logger.debug(\"total node Memory: %.0f gb\", total_node_mem)\n\n        # set up cores\n        executor_num = 1\n        core_per_executor = min(real_size / self.partition_size, n_core//3**2)* ((f + 1) // 2)\n\n        # keep driver core at most as the executor core \n\n        driver_core_num = max(1, min(n_core // 3 - core_per_executor*executor_num-1, f+1 )) \n        self.logger.debug(\"driver_core_num: %d\", driver_core_num)\n        self.logger.debug(\"core_per_executor : %d\", core_per_executor)\n\n        # set up memory \n        driver_memory =  ceil(min(real_size* ((f+1) // 2), total_node_mem/3**2*0.90))\n        executor_memory = ceil( max(1, min(min(self.exploding_factor, 5)*\n                                        max(self.partition_size, real_size // core_per_executor)*\n                                        core_per_executor * ((f + 1) // 2) / 0.4,\n                                        (total_node_mem / 3 - driver_memory / 0.9) * 0.90\n                                           )))\n        self.logger.debug(\"driver_memory: %.of GB\", driver_memory)\n        self.logger.debug(\"executor_memory : %.0f GB\", executor_memory)\n\n        resource = { \"driver_core_num\": None,\n                     \"driver_memory\": \"{0:, .0f}g\".format(driver_memory),\n                     \"executor_num\": executor_num,\n                     \"executor_memory\": \"{0:, .0f}g\".format(executor_memory),\n                     \"core_per_executor\": core_per_executor\n        }\n        self.__set_resource(resource=resource)\n\n    def optimize_yarn_resource(self, file_size =3, job_load=\"M\", \n                               zip_ratio=4, max_allowed_core=750, max_allowed_exe_mem=12):\n        \"\"\"\n        Calculate optimal resource for Yarn mode, update the resouce info based on the results \n\n        Args:\n            file_size (int, optional): the estimated file size from its pysical location.\n            job_load (str, optional): the estimated work load for the whole spark session.\n            zip_ratio (int, optional): the estimated zip_ratio of memory use to file size.\n            max_allowed_core (int, optional): the maximal allowed core number for the server.\n            max_allowed_exe_mem (int, optional): the maximal allowed memory for each executor.\n        \"\"\"\n\n        real_size = zip_ratio * file_size\n\n        # get load factor \n        f = self.__get_job_load_factor(job_load)\n        load_ratio = 2 * log2(f +1)\n\n        # Driver info \n        driver_core_num = min(25, max(2, ceil(real_size * load_ratio / 2)))\n        driver_memory = min(40, real_size * load_ratio)\n        self.logger.debug(\"driver_core_num %d\", driver_core_num)\n        self.logger.debug(\"driver_memory: %.0f GB\", driver_memory)\n\n        # executor Info \n        core_t = min(max_allowed_core, zip_ratio* ceil(file_size)*load_ratio*self.exploding_factor * 2 ** 3)\n\n        core_e = max(1, min(f, ceil( 25 / (f * self.partition_size / 0.4 + 300 / 2 ** 10 ))))\n        self.logger.debug(\"core_t: %d\", core_t)\n        self.logger.debug(\"core_e : %d\", core_e)\n\n        # limit the memory for each core below 10g due to server penalty for large memory \n        memory_per_core = min(max_allowed_exe_mem , max(self.partition_size* load_ratio, \n                                                        real_size*self.exploding_factor/ core_t) / 0.4 )\n\n        self.logger.debug(\"Memory per Core: % .0f GB\", memory_per_core)\n        executor_memory = min(25, ceil(core_e * memory_per_core * 1.1 + 300 / 2 ** 10))\n\n        executor_num = rounding_ceil(core_t / core_e, core_e)\n        core_per_executor = min(5, max(core_e, ceil(core_t // executor_num)))\n\n        # for large executor memory on HWX yarn, we need  to adjust it for cap max_allowed_exe_mem e.g 12g. since \n        # by experince, the server tends to 'penalize' large memory and slow down the program \n        # reduce core_per_executor until executor_memory is below max_allowed_exe_mem\n        # allow extra 25% buffer for core sharing memory\n\n        if executor_memory / 0.75 &gt; max_allowed_exe_mem:\n            self.logger.debug(\"Executor memory and number are adjusted to cap at %d GB memory limit\", max_allowed_exe_mem)\n\n            while core_per_executor &gt; 1:\n                # set a buffer for large use of cores \n                core_buffer = max_allowed_core / 750 \n                # the smaller the max allowed core number. the fewer core an executor can handle\n                core_per_executor = max(1, ceil(core_per_executor * 2 * core_buffer // 3) - 1 )\n                executor_memory = ceil(memory_per_core * core_per_executor)\n                # stop adjustment if executor memory drops below 10g\n                if executor_memory &lt;= max_allowed_exe_mem:\n                    break\n            else:\n                executor_memory = max_allowed_exe_mem\n\n        executor_num = rounding_ceil(core_t / core_per_executor, core_per_executor)\n\n        self.logger.debug(\"executor memory: %.0f GB\", executor_memory)\n        self.logger.debug(\"executor_num: %d\", executor_num)\n        self.logger.debug(\"core_per_executor : %d\", core_per_executor)\n\n        resource = { \"driver_core_num\": driver_core_num,\n                     \"driver_memory\": \"{0:, .0f}g\".format(driver_memory),\n                     \"executor_num\": executor_num,\n                     \"executor_memory\": \"{0:, .0f}g\".format(executor_memory),\n                     \"core_per_executor\": core_per_executor\n        }\n\n        self.__set_resource(resource)\n\n\n    def __get_job_load_factor(self, job_load):\n        \"\"\"\n        DEcide the load factor from job_load string\n\n        Args:\n            job_load (str): L-light, M-medium, H-heavy, MH-medium heavy, XH-extremely heavy \n\n        Return:\n            int: load factor \n        \"\"\"\n\n        if job_load.upper() == \"XH\":\n            load_factor = 4 \n            loadDesc = \"Extremly Heavy:  over 80% shuffle operations - aggregtes, sorts, joins and shuffle \"\n\n        elif job_load.upper() == \"H\":\n            load_factor = 3 \n            loadDesc = \"Heavy: 60% - 80% shuffle operations - Model Fitting. Quantile Calculations etc.\"\n\n        elif job_load.upper() == \"MH\":\n            load_factor = 2\n            loadDesc = \"Medium High : 40% - 60%  shuffle operations - joins , stattistics such as median etc. \"\n\n        elif job_load.upper() == \"L\":\n            load_factor = 0.75\n            loadDesc = \"Light :  &lt; 20% shuffle operations - I/O operations, Basic column calculations, Statistics such as mean, std etc \"\n\n        else:\n            if job_load.upper() != \"M\":\n                self.logger.warning(\"Requested job load %s invalid, use deafult Medium.\", job_load.upper())\n                job_load = \"M\"\n            load_factor =1\n            loadDesc = \"Medium (by default): 20%-40% shuffle operations - group level summarizations/aggregation, sorts, joins etc.\"\n\n        self.logger.info(\"Requested job load is %s.\", job_load.upper())\n        self.logger.info(\"Job Load is described as  %s.\", loadDesc)\n        return load_factor\n\n    @staticmethod\n    def get_file_size(filepath, filename):\n        \"\"\"\n        Obtain the file size from its physical location\n\n        Args:\n            filepath (str): the location of the file\n            filename (str): the name of the file \n\n        Returns \n            str: the file size in GB\n        \"\"\"\n        full_file = os.path.join(filepath, filename)\n        # get file size in MB\n        size_str = subprocess.getoutput(\"hadoop fs -du -s -h \" + full_file)\n        # return file size by GB \n        return str(ceil(float(size_str.split()[0])))\n\n    @staticmethod\n    def get_file_partition(filepath, filename):\n        \"\"\"\n        Obtain the file's current partition from its physical location \n\n        Args:\n            filepath (str): the location of the file\n            filename (str): the name of the file \n        Returns:\n           int: the total partition of the file \n        \"\"\"\n\n        full_file = os.path.join(filepath, filename)\n\n        # get the number of files\n\n        num_str = subprocess.getoutput(\"hadoop fs -count \" + full_file)\n        return int(num_str.split()[1]) - 1 \n\n    def tuning_read(self, filename, tuning=True, **kwargs):\n        \"\"\"\n        Read the file into Spark, and tune the dataframe by partition to match\n          total core number \n\n        Args:\n            filename (str): the full file path to be read from \n            tuning (bool): whether the dataframe needs to be automatically \n            tuned\n            **kwargs: parameters from tune_dataframe\n        Returns:\n            Spark Dataframe: the spark data frame read from spark, and tuned partition to match core numbers\n        \"\"\"\n        spark_df = self.spark.read.parquet(filename)\n        self.logger.info(\"Read Parquet File: %s\", filename)\n\n        if tuning:\n            spark_df = self.tune_dataframe(spark_df, **kwargs)\n        return spark_df\n\n\n    def tune_dataframe(self, spark_df, just_loaded=True):\n        \"\"\"\n        Tune the dataframe by partition to match total core number, so that it is multipliees of the core number\n\n        Args:\n            spark_df (Spark Dataframe):  Spark data frame to be tuned \n            just_loaded (bool): if True, the data frame is just loaded from the physical location, and the size \n            estimate is accurate;if False , the data frame has a execuation plan saved due to lazy evaluation,\n            and needs to be written to physical oaction to get accurate size estimate.\n\n        Returns:\n            Spark Dataframe: the tuned Spark Dataframe \n        \"\"\"\n\n        num_par = spark_df.rdd.getNumPartitions()\n        self.logger.info(\"The dataframe has %d Partitions\", num_par)\n\n        if just_loaded:\n            # for just loaded data frame, the get the data size directly \n            df_size = self.__get_df_size(spark_df)\n\n        else:\n            # if the dataframe is generated instead of being loaded directly, it needs to be \n            # written to physical location to get accurate estimate \n            user_dir = os.path.join(\"hdfs://cecldev/edl/appl/pyfrm/users/\", \n                                    os.environ[\"USER\"])\n            tmp_df = spark_df\n            tmp_file_loc = os.path.join(user_dir, \"tmp\")\n\n            # check if location exist , if not , create the folder \n\n            proc = subprocess.getoutput('hadoop fs -ls -d'+ tmp_file_loc)\n            x = re.findall(\"No such file or directory \", proc)\n            if len(x) &gt; 0:\n                subprocess.call(['hadoop', 'fs', '-mkdir', tmp_file_loc])\n\n            # save dataframe to tmp file \n            outfile = os.path.join(tmp_file_loc, \"temp_df\")\n            tmp_df.write.mode(\"overwrite\").parquet(outfile)\n            tmp_df = self.spark.read.parquet(outfile)\n            self.logger.info(\"data frame is written to %s to examine memory \\\n                             storage\", outfile)\n            df_size = self.__get_df_size(tmp_df)\n\n        self.logger.info(\"The dataframe takes an estimate of %s MB memory\", \"{:,.0f}\".format(df_size))\n\n        # get partition size . if partition size is too big or too small, adjust partition number \n\n        par_size = df_size / num_par\n        self.logger.info(\"Partition original size is n estimate of %d MB\", par_size) \n\n        if (par_size &lt;= 2 * self.partition_size * 2  ** 10) &amp; (par_size &gt;= self.partition_size * 2 ** 10 / 8):\n            # if partition size is moderate \n            new_par_num = self.total_exe_core\n            self.logger.info(\"Partition size is %d MB is moderate. Non need to change .\\n\"\n                             \"Partition number will be matched to the total executor core for higher perfomance\")\n        else:\n            if par_size &lt; self.partition_size * 2 ** 10 / 8:\n                self.logger.info(\"Partition size %d MB is too small. Repartition is recommended .\", par_size)\n            elif par_size &gt; 2 * self.partition_size * 2 ** 10:\n                self.logger.info(\"Partition size %d MB is too large, Repartition is recommended .\", par_size)\n\n            new_par_num = max(self.total_exe_core, \n                              min(\n                                  rounding_ceil(df_size / self.partition_size / 2 ** 10, self.total_exe_core), 2 ** 30))\n\n        # tuning partition first    \n        if num_par &lt; new_par_num:\n            self.logger.warning(\"The input data partition %d is smaller then the tuned partition %d. \\n\"\n                                \"The data is being repartitioned for higher performance\", num_par, new_par_num)\n\n            spark_df = spark_df.coalesce(new_par_num)\n\n        else:\n            self.logger.info(\"The input data partition matches the tuned partition %d. \\n\"\n                             \"The data is able to archive best performance\", self.total_exe_core)\n\n        return spark_df\n\n\n    def join_tuning_by(self, left_df, right_df, by_group=None):\n\n        if by_group is None:\n            by_group = []\n            left_df = self.tune_dataframe(left_df, just_loaded=False)\n            right_df = self.tune_dataframe(right_df, just_loaded=False)\n        else:\n            left_df = left_df.repartition(*by_group)\n            right_df = right_df.repartition(*by_group)\n        return [left_df, right_df]\n\n    def __get_df_size(self, spark_df):\n        \"\"\"\n        Get the dataframe estimated size from cached memory \n        Args:\n            spark_df:\n\n        Returns:\n\n        \"\"\"            \n\n        self.spark.catalog.clearCache()\n        spark_df.cache()\n        catalyst_plan = spark_df._jdf.queryExecution().logical()\n        # get size of the memory in Byte and return in MB \n        size = self.spark._jsparkSession.sessionState().executePlan(catalyst_plan) \\\n               .optimizedPlan().stats().sizeInBytes() / 1024 ** 2\n\n        spark_df.unpersit()\n        return size\n\n    @classmethod\n    def close_spark(cls):\n        \"\"\"\n        If spark is still working, stop spark before exit\n        \"\"\"\n\n        try:\n            cls.spark.sparkContext.stop()\n            cls.spark.stop()\n        except Exception:\n            print(\"Spark has already stopped. \\n\")\n        finally:\n            print(\"Spark is stopped. \\n\")\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__boost_conf","title":"<code>__boost_conf(driver_memory=None)</code>","text":"<p>Boosting configurations when higher Spark Perfomanse is requested</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __boost_conf(self, driver_memory=None):\n\n    \"\"\"\n       Boosting configurations when higher Spark Perfomanse is requested \n    \"\"\"\n    self.logger.info(\"Configuration Boosting for Large Scale Data\")\n\n    # failure handling \n    self.conf.set(\"spark.max.fetch.failures.per.stage\", 10) #default to 4\n\n    #rpc \n    self.conf.set(\"spark.rpc.io.serverThreads\", 64) # default 0 \n\n    # i/o\n    self.conf.set(\"spark.unsafe.sorter.spill.reader.buffer.size\", \"1m\") #default 32, recommend 1m \n    self.conf.set(\"spark.file.transferTo\", \"false\") #default true \n    self.conf.set(\"spark.io.compression.lz4.blockSize\", 512) #default 32\n\n    # shuffle \n\n    self.conf.set(\"spark.shuffle.registration.timeout\",\"2m\") #5000\n    self.conf.set(\"spark.shuffle.registration.maxAttempts\", 5 ) # default 3 \n\n    if driver_memory:\n        max_result_size = \"{:.0f}g\".format(ceil(max(1, float(driver_memory[:-1])/4)))\n        self.conf.set('spark.driver.maxResultSize', max_result_size)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__estimate_resource","title":"<code>__estimate_resource(driver_settings, exe_settings)</code>","text":"<p>Estimate resource total based on driver and executor settings</p> <p>Parameters:</p> Name Type Description Default <code>driver_settings</code> <code>dict</code> <p>driver core spark.driver.cores and memory spark.driver.memory </p> required <code>exe_settings</code> <code>dict</code> <p>executor information including spark.executor.cores,spark.executor.instances,</p> required Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __estimate_resource(self, driver_settings, exe_settings):\n    \"\"\"\n    Estimate resource total based on driver and executor settings\n\n    Args:\n        driver_settings (dict): driver core spark.driver.cores and memory spark.driver.memory \n        exe_settings (dict): executor information including spark.executor.cores,spark.executor.instances,\n        and spark.executor.memory \n    \"\"\"\n\n    # get total required core, each executor reserves 1 core for application master \n    total_core = driver_settings[\"spark.driver.cores\"] + (exe_settings[\"spark.executor.cores\"] + 1) * \\\n                 exe_settings[\"spark.executor.instances\"]\n\n    # get total required memory, which includes the memoryOverhead for both driver and executor \n\n    total_memory = int(driver_settings[\"spark.driver.memory\"][0:-1]) / (1-0.1) \\\n                   + (int(exe_settings[\"spark.executor.memory\"][0:-1]))* \\\n                   exe_settings[\"spark.executor.instances\"] / (1-0.1)\n    self.logger.info(\"Requested Total Estimate : {0:d} Core and {1:,.2f} GB Memory\"\n                     .format(total_core,total_memory))\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__get_df_size","title":"<code>__get_df_size(spark_df)</code>","text":"<p>Get the dataframe estimated size from cached memory  Args:     spark_df:</p> <p>Returns:</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __get_df_size(self, spark_df):\n    \"\"\"\n    Get the dataframe estimated size from cached memory \n    Args:\n        spark_df:\n\n    Returns:\n\n    \"\"\"            \n\n    self.spark.catalog.clearCache()\n    spark_df.cache()\n    catalyst_plan = spark_df._jdf.queryExecution().logical()\n    # get size of the memory in Byte and return in MB \n    size = self.spark._jsparkSession.sessionState().executePlan(catalyst_plan) \\\n           .optimizedPlan().stats().sizeInBytes() / 1024 ** 2\n\n    spark_df.unpersit()\n    return size\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__get_job_load_factor","title":"<code>__get_job_load_factor(job_load)</code>","text":"<p>DEcide the load factor from job_load string</p> <p>Parameters:</p> Name Type Description Default <code>job_load</code> <code>str</code> <p>L-light, M-medium, H-heavy, MH-medium heavy, XH-extremely heavy </p> required Return <p>int: load factor</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __get_job_load_factor(self, job_load):\n    \"\"\"\n    DEcide the load factor from job_load string\n\n    Args:\n        job_load (str): L-light, M-medium, H-heavy, MH-medium heavy, XH-extremely heavy \n\n    Return:\n        int: load factor \n    \"\"\"\n\n    if job_load.upper() == \"XH\":\n        load_factor = 4 \n        loadDesc = \"Extremly Heavy:  over 80% shuffle operations - aggregtes, sorts, joins and shuffle \"\n\n    elif job_load.upper() == \"H\":\n        load_factor = 3 \n        loadDesc = \"Heavy: 60% - 80% shuffle operations - Model Fitting. Quantile Calculations etc.\"\n\n    elif job_load.upper() == \"MH\":\n        load_factor = 2\n        loadDesc = \"Medium High : 40% - 60%  shuffle operations - joins , stattistics such as median etc. \"\n\n    elif job_load.upper() == \"L\":\n        load_factor = 0.75\n        loadDesc = \"Light :  &lt; 20% shuffle operations - I/O operations, Basic column calculations, Statistics such as mean, std etc \"\n\n    else:\n        if job_load.upper() != \"M\":\n            self.logger.warning(\"Requested job load %s invalid, use deafult Medium.\", job_load.upper())\n            job_load = \"M\"\n        load_factor =1\n        loadDesc = \"Medium (by default): 20%-40% shuffle operations - group level summarizations/aggregation, sorts, joins etc.\"\n\n    self.logger.info(\"Requested job load is %s.\", job_load.upper())\n    self.logger.info(\"Job Load is described as  %s.\", loadDesc)\n    return load_factor\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__init__","title":"<code>__init__(app_name='optimize Spark', spark_queue='default', spark_master='yarn', reset_port=False, log_yaml='configs/logging.yml', config_properties=None, timezone='UTC', **kwargs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>The application name that identfies the spark job</p> <code>'optimize Spark'</code> <code>spark_queue</code> <code>str</code> <p>the queue that is assigned to the user, it is required parameter and must be specified .</p> <code>'default'</code> <code>spark_master</code> <code>str</code> <p>yarn for cluster, and local for client, in which only the edge node isused</p> <code>'yarn'</code> <code>reset_port</code> <code>bool</code> <p>reset port to 0 if True, due to Jupyter hub restriction </p> <code>False</code> <code>log_yaml</code> <code>str</code> <p>log yaml configuration file for logging. By defaut, it uses an undefined setting.</p> <code>'configs/logging.yml'</code> <code>config_properties</code> <code>dict</code> <p>additional configurations used for spark.conf. when  config_properties={\"spark.sql.autoBroadcastJoinThreshold\":-1}, the configuration property spark.sql.autoBroadcastJoinThreshold is set to be -1</p> <code>None</code> <code>timezone</code> <code>str</code> <p>the timezone for the spark session, UTC, EST, PST, etc </p> <code>'UTC'</code> <code>**kwargs</code> <code>dict</code> <p>additional parameters for optimize resources , inclusing file_size, job_load,  exploding_factor,zip_ratio etc.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt;&gt; # loading the OptimizeSpark class from the module optimize_spark \n&gt;&gt;&gt;&gt; from op_spark import * \n&gt;&gt;&gt;&gt; # apply the logging setting defined in logging.yml\n&gt;&gt;&gt;&gt; root_path =\"/home/CODE_DIR\"\n&gt;&gt;&gt;&gt; opts = OptimizeSpark(app_name=\"optimize spark\", spark_queue=\"default\",                          spark_master=\"yarn\", log_yaml=os.path.join(root_path,\"configs/logging.yml\")\n&gt;&gt;&gt;&gt; opts \n&lt;op_spark.optimize_spark.OptimizeSpark at 087hyt677&gt;\n</code></pre> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __init__(self, app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\", \n                   reset_port=False, log_yaml=\"configs/logging.yml\",\n                   config_properties=None, timezone='UTC', **kwargs) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        app_name (str, optional): The application name that identfies the spark job\n        spark_queue (str, optional): the queue that is assigned to the user, it is required parameter and must be specified .\n        spark_master (str, optional): yarn for cluster, and local for client, in which only the edge node isused\n        reset_port (bool, optional): reset port to 0 if True, due to Jupyter hub restriction \n        log_yaml (str, optional): log yaml configuration file for logging. By defaut, it uses an undefined setting.\n        config_properties (dict, optional): additional configurations used for spark.conf. when \n            config_properties={\"spark.sql.autoBroadcastJoinThreshold\":-1},\n            the configuration property spark.sql.autoBroadcastJoinThreshold\n            is set to be -1\n        timezone (str, optional): the timezone for the spark session, UTC, EST, PST, etc \n        **kwargs (dict): additional parameters for optimize resources , inclusing file_size, job_load,\n             exploding_factor,zip_ratio etc.\n\n    Examples:\n        &gt;&gt;&gt;&gt; # loading the OptimizeSpark class from the module optimize_spark \n        &gt;&gt;&gt;&gt; from op_spark import * \n        &gt;&gt;&gt;&gt; # apply the logging setting defined in logging.yml\n        &gt;&gt;&gt;&gt; root_path =\"/home/CODE_DIR\"\n        &gt;&gt;&gt;&gt; opts = OptimizeSpark(app_name=\"optimize spark\", spark_queue=\"default\",\\\n                      spark_master=\"yarn\", log_yaml=os.path.join(root_path,\"configs/logging.yml\")\n        &gt;&gt;&gt;&gt; opts \n        &lt;op_spark.optimize_spark.OptimizeSpark at 087hyt677&gt;\n    \"\"\"\n\n    # loading logging info \n\n    if config_properties is None:\n        config_properties = {}\n\n    # get logging info \n    self.logger = set_logger_format(config_file=log_yaml, classname=__name__)\n\n    self.conf = SparkConf().setAppName(app_name)\n    # initialize resourse settings\n    self.__init_resource()\n\n    # Switch on yarn or local \n\n    if spark_master.lower() not in [\"yarn\", \"local\"]:\n        self.logger.error(\"The input spark master %s is invalid. \"\n                          \"it must be one of the following: Local or Yarn.\\n\"\n                          \"No Spark Session is initialized.\", spark_master)\n\n        return\n    elif spark_master.lower() != \"yarn\":\n        # parallelism selection for local mode \n        self.set_spark_master(\"local\")\n        self.logger.info(\"Local Mode\")\n\n    else:\n        self.set_spark_master(spark_master)\n        self.logger.info(\"Yarn Mode\")\n\n    self.exploding_factor = 1\n    self.default_setting = True \n    self.conf.setMaster(self.spark_master)\n    self.conf.set(\"spark.yarn.queue\", spark_queue)\n    self.__set_conf(config_properties, reset_port, timezone)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__init_resource","title":"<code>__init_resource()</code>","text":"<p>Initialize the resources and default values from spark</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __init_resource(self):\n    \"\"\"\n    Initialize the resources and default values from spark\n    \"\"\"\n    self.driver_memory = None \n    self.driver_core_num = None \n    self.executor_num = None \n    self.core_per_executor = None \n    self.executor_memory = None \n    self.partition_size = 128/2 ** 10 \n\n    # default list for spark on these resources\n\n    self.defaultConf = {\n      \"spark.driver.cores\": 1,\n      \"spark.driver.memory\": \"1g\",\n      \"spark.executor.memory\": \"1g\",\n      \"spark.executor.cores\": 1,\n      \"spark.executor.instances\": 2\n    }\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__set_conf","title":"<code>__set_conf(config_properties, reset_port, timezone)</code>","text":"<p>Set up all normal configurations for spark setup </p> <p>Parameters:</p> Name Type Description Default <code>config_properties</code> <code>dict</code> <p>a dictionary of config properties to be set additionally </p> required <code>reset_port</code> <code>bool</code> <p>Setting for jupiterHub restriction</p> required Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __set_conf(self, config_properties, reset_port, timezone):\n    \"\"\"\n    Set up all normal configurations for spark setup \n\n    Args:\n        config_properties (dict):  a dictionary of config properties to be set additionally \n        reset_port (bool): Setting for jupiterHub restriction\n    \"\"\"\n    utz = \"-Duster.timesone=\" + timezone.upper().strip()\n    self.conf.set(\"spark.sql.session.timeZone\", timezone.upper())\n    self.conf.set(\"spark.executor.extraJavaOptions\", utz)\n    self.conf.set(\"spark.driver.extraJavaOptions\", utz)\n    self.logger.info(\"Timezone set to be %s\", timezone.upper())\n\n    # set logging settings\n    self.conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n    self.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n    self.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n    self.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\n\n    # multiple contexts \n    self.conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n    self.conf.set(\"spark.storage.cleanupFilesAfterExecutorExit\", \"true\")\n    self.conf.set(\"spark.worker.cleanup.enabled\", \"true\")\n    self.conf.set(\"spark.worker.cleanup.interval\", 5)\n    self.conf.set(\"spark.worker.cleanup.appDataTtl\", 5)\n    self.conf.set(\"spark.python.use.daemon\", 'false')\n\n    self.conf.set('spark.sql.autoBroadcastJoinThreshold', '-1')\n\n    if len(config_properties) &gt; 0:\n        for confKey, confValue in config_properties.items():\n            self.conf.set(confKey, confValue)\n\n    # set up port \n\n    if reset_port:\n        self.conf.set(\"spark.ui.port\", 0)\n    import os \n\n    self.conf.set('spark.yarn.staingDir', os.environ[\"SPARK_YARN_SG_DIR\"])\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__set_resource","title":"<code>__set_resource(resource)</code>","text":"<p>Set up resources to the Spark Session</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>dict</code> <p>spark resources to start spark session, it lists  [driver_core_num, driver_memory, executor_num, executor_memory,core_per_executor].</p> required Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __set_resource(self, resource):\n    \"\"\"\n    Set up resources to the Spark Session\n\n    Args:\n        resource (dict): spark resources to start spark session, it lists\n             [driver_core_num, driver_memory, executor_num, executor_memory,core_per_executor].\n    \"\"\"\n    self.default_setting = False \n    self.driver_memory = resource['driver_memory']\n    self.executor_num = resource[\"executor_num\"]\n    self.driver_core_num = resource['driver_core_num']\n    self.core_per_executor = resource['core_per_executor']\n    self.executor_memory = resource['executor_memory']\n\n    # set up total core for perfomance tuning \n\n    self.total_exe_core = self.executor_num* self.core_per_executor\n\n    # set up shuffle partition for joining and aggregation \n\n    self.conf.set(\"spark.sql.shuffle.partitions\", self.total_exe_core) # default 200 \n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.__set_resource_conf","title":"<code>__set_resource_conf()</code>","text":"<p>Based on the resource information , set up the configuration settings for reources  Returns:</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def __set_resource_conf(self):\n    \"\"\"\n    Based on the resource information , set up the configuration settings for reources \n    Returns:\n    \"\"\"\n\n    # driver settings\n\n    self.conf.set(\"spark.driver.memory\", self.driver_memory)\n    if self.spark_master.lower() == \"yarn\":\n        self.conf.set(\"spark.driver.cores\", self.driver_core_num)\n\n    # Executor settings\n\n    self.conf.set(\"spark.executor.instances\", self.executor_num)\n    self.conf.set(\"spark.executor.cores\", self.core_per_executor)\n    self.conf.set(\"spark.executor.memory\", self.executor_memory)\n\n    try:\n        varobj = getattr(self, \"exploding_factor\")\n        exploding_set = True \n\n    except AttributeError as error:\n        exploding_set = False \n        self.logger.warning(error)\n        self.logger.info(\"exploding_factor is taken as default 1.\")\n\n    if exploding_set:\n        if self.exploding_factor &gt; 10:\n            # offheap \n            self.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n            mem_num = float(re.findall('\\d+', self.executor_memory)[0])\n            mem_unit = re.findall(\"\\D\", self.executor_memory)\n\n            # overhead memory  should be at least 384\n\n            if len(mem_unit) == 0:\n                memory_overhead = max(384, mem_num / 2 **10)/4 \n            elif mem_unit[0].lower() == 'm':\n                memory_overhead = max(384, mem_num) / 4\n            else:\n                memory_overhead = max(384, mem_num*2**10)/4\n\n            heap_size = \"{:.0f}m\".format(ceil(memory_overhead*0.6))\n            self.conf.set(\"spark.memory.offHeap.size\", heap_size)\n            # overhead = offHeap + 0.25* exe_num . for spark.3.0+, not need to include\n            # before spark 2, overhead needs to include offHeap\n            overhead = \"{:.0f}m\".format(memory_overhead)\n            self.conf.set(\"spark.executor.memoryOverhead\",overhead) # default is 10% of executor memory \n            self.logger.info(\"spark.executor.memoryOverhead is reset to %s\", overhead)\n            self.logger.info(\"spark.memory.offHeap.size is reset to %s \", heap_size)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.close_spark","title":"<code>close_spark()</code>  <code>classmethod</code>","text":"<p>If spark is still working, stop spark before exit</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>@classmethod\ndef close_spark(cls):\n    \"\"\"\n    If spark is still working, stop spark before exit\n    \"\"\"\n\n    try:\n        cls.spark.sparkContext.stop()\n        cls.spark.stop()\n    except Exception:\n        print(\"Spark has already stopped. \\n\")\n    finally:\n        print(\"Spark is stopped. \\n\")\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.get_file_partition","title":"<code>get_file_partition(filepath, filename)</code>  <code>staticmethod</code>","text":"<p>Obtain the file's current partition from its physical location </p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the location of the file</p> required <code>filename</code> <code>str</code> <p>the name of the file </p> required <p>Returns:    int: the total partition of the file</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>@staticmethod\ndef get_file_partition(filepath, filename):\n    \"\"\"\n    Obtain the file's current partition from its physical location \n\n    Args:\n        filepath (str): the location of the file\n        filename (str): the name of the file \n    Returns:\n       int: the total partition of the file \n    \"\"\"\n\n    full_file = os.path.join(filepath, filename)\n\n    # get the number of files\n\n    num_str = subprocess.getoutput(\"hadoop fs -count \" + full_file)\n    return int(num_str.split()[1]) - 1 \n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.get_file_size","title":"<code>get_file_size(filepath, filename)</code>  <code>staticmethod</code>","text":"<p>Obtain the file size from its physical location</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the location of the file</p> required <code>filename</code> <code>str</code> <p>the name of the file </p> required <p>Returns      str: the file size in GB</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>@staticmethod\ndef get_file_size(filepath, filename):\n    \"\"\"\n    Obtain the file size from its physical location\n\n    Args:\n        filepath (str): the location of the file\n        filename (str): the name of the file \n\n    Returns \n        str: the file size in GB\n    \"\"\"\n    full_file = os.path.join(filepath, filename)\n    # get file size in MB\n    size_str = subprocess.getoutput(\"hadoop fs -du -s -h \" + full_file)\n    # return file size by GB \n    return str(ceil(float(size_str.split()[0])))\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.get_resource_info","title":"<code>get_resource_info()</code>","text":"<p>Provide an estimate of the resouse settings Returns:    dict: a dictionary of resources, i.e., {\"spark.driver.cores\":1,               \"spark.driver.memory\": \"1g\",               \"spark.executor.memory\": \"1g\",               \"spark.executor.cores\": 1,               \"spark.executor.instances\": 2}</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def get_resource_info(self):\n    \"\"\"\n    Provide an estimate of the resouse settings\n    Returns:\n       dict: a dictionary of resources, i.e., {\"spark.driver.cores\":1,\n                  \"spark.driver.memory\": \"1g\",\n                  \"spark.executor.memory\": \"1g\",\n                  \"spark.executor.cores\": 1,\n                  \"spark.executor.instances\": 2}\n    \"\"\"\n\n    driver_settings = {\"spark.driver.cores\": self.driver_core_num,\n                       \"spark.driver.memory\": self.driver_memory}\n\n    exe_settings = {\"spark.executor.memory\": self.executor_memory,\n                    \"spark.executor.cores\": self.core_per_executor,\n                    \"spark.executor.instances\": self.executor_num}\n\n    for res, setVal in driver_settings.items():\n        if setVal is None:\n            driver_settings[res] = self.defaultConf[res]\n            self.logger.warning(\"Driver Settings: %s not set, default is : %s\", res, self.defaultConf[res])\n        else:\n            self.logger.info(\"Driver settings: %s: %s\", res, setVal)\n    for res, setVal in exe_settings.items():\n        if setVal is None:\n            driver_settings[res] = self.defaultConf[res]\n            self.logger.warning(\"Executor Settings: %s not set, default is : %s\", res, self.defaultConf[res])\n        else:\n            self.logger.info(\"Executor settings: %s: %s\", res, setVal)\n\n    self.__estimate_resource(driver_settings, exe_settings)\n\n    return {\n                  \"spark.driver.cores\": self.driver_core_num,\n                  \"spark.driver.memory\": self.driver_memory,\n                  \"spark.executor.memory\": self.executor_memory,\n                  \"spark.executor.cores\": self.core_per_executor,\n                  \"spark.executor.instances\": self.executor_num\n    }\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.get_spark_info","title":"<code>get_spark_info()</code>","text":"<p>Provide an overview of the spark useful links used for development or moniter purpose</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def get_spark_info(self):\n    \"\"\"\n    Provide an overview of the spark useful links used for development or moniter purpose \n    \"\"\"\n\n    sc = self.spark.sparkContext \n\n    # get UI address \n    uiUrl = \"https://{hostname}:12443/gateway/default/yarn/proxy/\" + sc.applicationId\n    self.logger.info(\"UI address: \\n %s \\n\", uiUrl)\n\n    # get scheduler address \n\n    scdlrUrl = \"https://{hostname}:61443/gateway/default/yarnuiv2/redirect#/yarn-app\" + sc.applicationId\n\n    self.logger.info(\"Scheduler address: \\n %s \\n\", scdlrUrl)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.get_spark_session","title":"<code>get_spark_session()</code>","text":"<p>Get the Spark Session generated  Returns:     SparkSession: the spark session created by the proceeding operations on OptimizedSpark</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def get_spark_session(self):\n    \"\"\"\n     Get the Spark Session generated \n     Returns:\n         SparkSession: the spark session created by the proceeding operations on OptimizedSpark\n    \"\"\"\n    return self.spark\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.optimize_local_resource","title":"<code>optimize_local_resource(file_size=3, job_load='M', zip_ratio=4)</code>","text":"<p>Calculate optimal resource for Local mode, update the resource info based on the results </p> <p>Parameters:</p> Name Type Description Default <code>file_size</code> <code>int</code> <p>the estimated file size from its physical location.</p> <code>3</code> <code>job_load</code> <code>str</code> <p>the estimated work load for the whole spark session.</p> <code>'M'</code> <code>zip_ratio</code> <code>int</code> <p>the estimated zip ration of memory use to file size .</p> <code>4</code> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def optimize_local_resource(self, file_size=3, job_load=\"M\", zip_ratio=4):\n    \"\"\"\n    Calculate optimal resource for Local mode, update the resource info based on the results \n\n    Args:\n        file_size (int, optional):  the estimated file size from its physical location.\n        job_load (str, optional): the estimated work load for the whole spark session.\n        zip_ratio (int, optional): the estimated zip ration of memory use to file size .\n    \"\"\"\n\n    real_size = zip_ratio * file_size \n\n    # get load factor \n    f = self.__get_job_factor(job_load)\n    # get total core on the node \n    n_core = int(subprocess.getoutput(\"nproc\"))\n    self.logger.debug(\"total core : %d\", n_core)\n\n    # get total memory of the node as 1/3 of total free memory, with MB as unit\n    total_node_mem = float(re.compile('\\D').sub('', subprocess.getoutput(\"grep MemFree /proc/meminfo\"))) / (1024**2)\n    self.logger.debug(\"total node Memory: %.0f gb\", total_node_mem)\n\n    # set up cores\n    executor_num = 1\n    core_per_executor = min(real_size / self.partition_size, n_core//3**2)* ((f + 1) // 2)\n\n    # keep driver core at most as the executor core \n\n    driver_core_num = max(1, min(n_core // 3 - core_per_executor*executor_num-1, f+1 )) \n    self.logger.debug(\"driver_core_num: %d\", driver_core_num)\n    self.logger.debug(\"core_per_executor : %d\", core_per_executor)\n\n    # set up memory \n    driver_memory =  ceil(min(real_size* ((f+1) // 2), total_node_mem/3**2*0.90))\n    executor_memory = ceil( max(1, min(min(self.exploding_factor, 5)*\n                                    max(self.partition_size, real_size // core_per_executor)*\n                                    core_per_executor * ((f + 1) // 2) / 0.4,\n                                    (total_node_mem / 3 - driver_memory / 0.9) * 0.90\n                                       )))\n    self.logger.debug(\"driver_memory: %.of GB\", driver_memory)\n    self.logger.debug(\"executor_memory : %.0f GB\", executor_memory)\n\n    resource = { \"driver_core_num\": None,\n                 \"driver_memory\": \"{0:, .0f}g\".format(driver_memory),\n                 \"executor_num\": executor_num,\n                 \"executor_memory\": \"{0:, .0f}g\".format(executor_memory),\n                 \"core_per_executor\": core_per_executor\n    }\n    self.__set_resource(resource=resource)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.optimize_resources","title":"<code>optimize_resources(file_size=3, job_load='M', exploding_factor=1, zip_ratio=4, overheap_adjust=False, **kwargs)</code>","text":"<p>Calculate optimal resource based on know info </p> <p>Parameters:</p> Name Type Description Default <code>file_size</code> <code>int</code> <p>the estimated file size from its pysical location </p> <code>3</code> <code>job_load</code> <code>str</code> <p>the estimated work load for the whole spark session </p> <code>'M'</code> <code>zip_ratio(int)</code> <p>the estimated zip ratio of memory to file size </p> required <code>exploding_factor</code> <code>int</code> <p>if the input data will be transformed and expanded exponentially, this parameter   helps to estimate how many times the data will be exploded </p> <code>1</code> <code>overheap_adjust</code> <code>bool</code> <p>whether overlap needs to be adjusted </p> <code>False</code> <code>**kwargs</code> <p>additional parameters for optimal_local_resource or optimal_yarn_resource</p> <code>{}</code> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def optimize_resources(self, file_size=3, job_load=\"M\", \n                       exploding_factor=1, zip_ratio=4, overheap_adjust=False, **kwargs):\n\n    \"\"\"\n    Calculate optimal resource based on know info \n\n    Args:\n        file_size (int): the estimated file size from its pysical location \n        job_load (str): the estimated work load for the whole spark session \n        zip_ratio(int): the estimated zip ratio of memory to file size \n        exploding_factor (int): if the input data will be transformed and expanded exponentially, this parameter \n             helps to estimate how many times the data will be exploded \n        overheap_adjust (bool): whether overlap needs to be adjusted \n        **kwargs: additional parameters for optimal_local_resource or optimal_yarn_resource\n    \"\"\"\n\n    self.exploding_factor = exploding_factor\n    self.overheap_adjust = overheap_adjust\n    # redesign file size if provided by string \n    if type(file_size) == int:\n        f_size = file_size\n    else:\n        self.logger.warning(\"File Size is provided by a string %s, please verify.\", file_size)\n        f_num = float(re.findall(\"\\d+\", file_size)[0])\n        f_unit = re.findall(\"\\D\", file_size)\n        if len(f_unit) == 0:\n            f_size = ceil(f_num)\n        elif f_unit[0].lower() == 'm':\n            f_size = ceil(f_num/1024)\n        elif f_unit[0].lower() == 'g':\n            f_size = int(ceil(f_num))\n        else:\n            self.logger.error(\" Strings for the file size can only be parsed with unit 'm'/'g' for 'MB'/'GB'. \\n \"\n                              \"Otherwise, please use intergers for file size in GB \")\n\n    self.logger.info(\"Initializing OptimizeSpark with Job Size %d GB, Job Load %s\", f_size, job_load)\n    self.logger.info(\"Estimated Real Data Size %d GB\", f_size * zip_ratio)\n    if self.spark_master.lower() != \"yarn\":\n        # parallelism selection for local mode \n        if exploding_factor &gt; 5:\n            self.logger.warning(\"Local resource is not abundant for large datasets with exploding_factor &gt; 5.\"\n                                \"Id the spark session fails, plaese consider Yarn mode. \\n\")\n        self.optimize_local_resources(file_size=file_size, job_load=job_load, zip_ratio=zip_ratio)\n        self.set_spark_master(\"local\" + \"[\" + str(self.driver_core_num) + \"]\")\n    else:\n        self.optimize_yarn_resources(file_size=file_size, job_load=job_load, zip_ratio=zip_ratio, **kwargs)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.optimize_yarn_resource","title":"<code>optimize_yarn_resource(file_size=3, job_load='M', zip_ratio=4, max_allowed_core=750, max_allowed_exe_mem=12)</code>","text":"<p>Calculate optimal resource for Yarn mode, update the resouce info based on the results </p> <p>Parameters:</p> Name Type Description Default <code>file_size</code> <code>int</code> <p>the estimated file size from its pysical location.</p> <code>3</code> <code>job_load</code> <code>str</code> <p>the estimated work load for the whole spark session.</p> <code>'M'</code> <code>zip_ratio</code> <code>int</code> <p>the estimated zip_ratio of memory use to file size.</p> <code>4</code> <code>max_allowed_core</code> <code>int</code> <p>the maximal allowed core number for the server.</p> <code>750</code> <code>max_allowed_exe_mem</code> <code>int</code> <p>the maximal allowed memory for each executor.</p> <code>12</code> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def optimize_yarn_resource(self, file_size =3, job_load=\"M\", \n                           zip_ratio=4, max_allowed_core=750, max_allowed_exe_mem=12):\n    \"\"\"\n    Calculate optimal resource for Yarn mode, update the resouce info based on the results \n\n    Args:\n        file_size (int, optional): the estimated file size from its pysical location.\n        job_load (str, optional): the estimated work load for the whole spark session.\n        zip_ratio (int, optional): the estimated zip_ratio of memory use to file size.\n        max_allowed_core (int, optional): the maximal allowed core number for the server.\n        max_allowed_exe_mem (int, optional): the maximal allowed memory for each executor.\n    \"\"\"\n\n    real_size = zip_ratio * file_size\n\n    # get load factor \n    f = self.__get_job_load_factor(job_load)\n    load_ratio = 2 * log2(f +1)\n\n    # Driver info \n    driver_core_num = min(25, max(2, ceil(real_size * load_ratio / 2)))\n    driver_memory = min(40, real_size * load_ratio)\n    self.logger.debug(\"driver_core_num %d\", driver_core_num)\n    self.logger.debug(\"driver_memory: %.0f GB\", driver_memory)\n\n    # executor Info \n    core_t = min(max_allowed_core, zip_ratio* ceil(file_size)*load_ratio*self.exploding_factor * 2 ** 3)\n\n    core_e = max(1, min(f, ceil( 25 / (f * self.partition_size / 0.4 + 300 / 2 ** 10 ))))\n    self.logger.debug(\"core_t: %d\", core_t)\n    self.logger.debug(\"core_e : %d\", core_e)\n\n    # limit the memory for each core below 10g due to server penalty for large memory \n    memory_per_core = min(max_allowed_exe_mem , max(self.partition_size* load_ratio, \n                                                    real_size*self.exploding_factor/ core_t) / 0.4 )\n\n    self.logger.debug(\"Memory per Core: % .0f GB\", memory_per_core)\n    executor_memory = min(25, ceil(core_e * memory_per_core * 1.1 + 300 / 2 ** 10))\n\n    executor_num = rounding_ceil(core_t / core_e, core_e)\n    core_per_executor = min(5, max(core_e, ceil(core_t // executor_num)))\n\n    # for large executor memory on HWX yarn, we need  to adjust it for cap max_allowed_exe_mem e.g 12g. since \n    # by experince, the server tends to 'penalize' large memory and slow down the program \n    # reduce core_per_executor until executor_memory is below max_allowed_exe_mem\n    # allow extra 25% buffer for core sharing memory\n\n    if executor_memory / 0.75 &gt; max_allowed_exe_mem:\n        self.logger.debug(\"Executor memory and number are adjusted to cap at %d GB memory limit\", max_allowed_exe_mem)\n\n        while core_per_executor &gt; 1:\n            # set a buffer for large use of cores \n            core_buffer = max_allowed_core / 750 \n            # the smaller the max allowed core number. the fewer core an executor can handle\n            core_per_executor = max(1, ceil(core_per_executor * 2 * core_buffer // 3) - 1 )\n            executor_memory = ceil(memory_per_core * core_per_executor)\n            # stop adjustment if executor memory drops below 10g\n            if executor_memory &lt;= max_allowed_exe_mem:\n                break\n        else:\n            executor_memory = max_allowed_exe_mem\n\n    executor_num = rounding_ceil(core_t / core_per_executor, core_per_executor)\n\n    self.logger.debug(\"executor memory: %.0f GB\", executor_memory)\n    self.logger.debug(\"executor_num: %d\", executor_num)\n    self.logger.debug(\"core_per_executor : %d\", core_per_executor)\n\n    resource = { \"driver_core_num\": driver_core_num,\n                 \"driver_memory\": \"{0:, .0f}g\".format(driver_memory),\n                 \"executor_num\": executor_num,\n                 \"executor_memory\": \"{0:, .0f}g\".format(executor_memory),\n                 \"core_per_executor\": core_per_executor\n    }\n\n    self.__set_resource(resource)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.set_spark_master","title":"<code>set_spark_master(spark_master)</code>","text":"<p>Set spark master to 'local' or 'yarn'</p> <p>Parameters:</p> Name Type Description Default <code>spark_master</code> <code>str</code> <p>\"local\" or \"yarn\" mode.</p> required Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def set_spark_master(self, spark_master):\n    \"\"\"\n    Set spark master to 'local' or 'yarn'\n\n    Args:\n        spark_master (str): \"local\" or \"yarn\" mode.\n    \"\"\"\n\n    self.spark_master = spark_master\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.start_optimal_spark","title":"<code>start_optimal_spark(resource=None, **kwargs)</code>","text":"<p>Start a Spark Session with provided resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>dict</code> <p>spark resources to start spark session. It lists       [driver_core_num, driver_memory, executor_num, executor_memory, core_per_executor]. if not provided       start_optimal_spark will calculate the optimal resources, apply boosting configurations,      and start spark sessions.      if provided, start_optimal_spark will apply the boosting configuraton, and provided resource to start       spark session</p> <code>None</code> <code>**kwargs</code> <p>parameters used in optimize_resources </p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize_spark\", spark_queue=\"default\",\n               spark_master=\"yarn\").start_optimal_spark(file_size=1, job_load='m')\n</code></pre> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def start_optimal_spark(self, resource=None, **kwargs):\n    \"\"\"\n    Start a Spark Session with provided resource.\n\n    Args:\n        resource (dict): spark resources to start spark session. It lists \n                 [driver_core_num, driver_memory, executor_num, executor_memory, core_per_executor]. if not provided \n                 start_optimal_spark will calculate the optimal resources, apply boosting configurations,\n                 and start spark sessions.\n                 if provided, start_optimal_spark will apply the boosting configuraton, and provided resource to start \n                 spark session\n\n        **kwargs: parameters used in optimize_resources \n\n    Examples:\n        &gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize_spark\", spark_queue=\"default\",\n                       spark_master=\"yarn\").start_optimal_spark(file_size=1, job_load='m')\n    \"\"\"\n\n    if len(kwargs) &gt; 0:\n        self.optimize_resources(**kwargs)\n\n    if resource is None:\n        self.__boost_conf(self.driver_memory)\n        return self.start_spark()\n\n    else:\n        self.__boost_conf(resource[\"driver_memory\"])\n        return self.start_spark(resource=resource)\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.start_spark","title":"<code>start_spark(resource=None)</code>","text":"<p>Start a Spark Session with provided resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>dict</code> <p>spark resources to start spark session. It lists </p> <code>None</code> <p>Returns:</p> Name Type Description <code>SparkSession</code> <p>the spark Session to be used for spark applications</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;&gt; my_resource = { 'driver_core_num': 1, 'driver_memory:'1g', 'executor_num': 3,\n                     'executor_memory': '2g', 'core_per_executor': 3}            &gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\").start_spark(my_resource)\n</code></pre> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def start_spark(self, resource: dict =None):\n    \"\"\"\n    Start a Spark Session with provided resource.\n\n    Args:\n        resource (dict): spark resources to start spark session. It lists \n        [driver_core_num, driver_memory, executor_num, executor_memory, core_per_executor]. if not provided \n        start_spark will use the default spark settings.\n\n    Returns:\n        SparkSession: the spark Session to be used for spark applications\n\n    Examples:\n        &gt;&gt;&gt;&gt; my_resource = { 'driver_core_num': 1, 'driver_memory:'1g', 'executor_num': 3,\n                             'executor_memory': '2g', 'core_per_executor': 3}\\\n        &gt;&gt;&gt;&gt; spark = OptimizeSpark(app_name=\"optimize Spark\", spark_queue=\"default\", spark_master=\"yarn\").start_spark(my_resource)\n\n    \"\"\"\n\n    if resource is None :\n        resource = {}\n    if len(resource) &gt; 0:\n        self.__set_resource(resource)\n\n    if not self.default_setting:\n        self.logger.info(\"Initializing OptimizeSpark with Customized Resources\")\n        self.__set_resource_conf()\n\n    else:\n        self.logger.info(\"Initializing OptimizeSpark with Default Resources\")\n\n    self.spark = SparkSession.builder.config(conf=self.conf).enableHiveSupport().getOrCreate()\n    self.spark.sparkContext.setLogLevel(\"ERROR\")\n    self.get_spark_info()\n    self.get_resource_info()\n    return self.spark\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.tune_dataframe","title":"<code>tune_dataframe(spark_df, just_loaded=True)</code>","text":"<p>Tune the dataframe by partition to match total core number, so that it is multipliees of the core number</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>Spark Dataframe</code> <p>Spark data frame to be tuned </p> required <code>just_loaded</code> <code>bool</code> <p>if True, the data frame is just loaded from the physical location, and the size </p> <code>True</code> <p>Returns:</p> Type Description <p>Spark Dataframe: the tuned Spark Dataframe</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def tune_dataframe(self, spark_df, just_loaded=True):\n    \"\"\"\n    Tune the dataframe by partition to match total core number, so that it is multipliees of the core number\n\n    Args:\n        spark_df (Spark Dataframe):  Spark data frame to be tuned \n        just_loaded (bool): if True, the data frame is just loaded from the physical location, and the size \n        estimate is accurate;if False , the data frame has a execuation plan saved due to lazy evaluation,\n        and needs to be written to physical oaction to get accurate size estimate.\n\n    Returns:\n        Spark Dataframe: the tuned Spark Dataframe \n    \"\"\"\n\n    num_par = spark_df.rdd.getNumPartitions()\n    self.logger.info(\"The dataframe has %d Partitions\", num_par)\n\n    if just_loaded:\n        # for just loaded data frame, the get the data size directly \n        df_size = self.__get_df_size(spark_df)\n\n    else:\n        # if the dataframe is generated instead of being loaded directly, it needs to be \n        # written to physical location to get accurate estimate \n        user_dir = os.path.join(\"hdfs://cecldev/edl/appl/pyfrm/users/\", \n                                os.environ[\"USER\"])\n        tmp_df = spark_df\n        tmp_file_loc = os.path.join(user_dir, \"tmp\")\n\n        # check if location exist , if not , create the folder \n\n        proc = subprocess.getoutput('hadoop fs -ls -d'+ tmp_file_loc)\n        x = re.findall(\"No such file or directory \", proc)\n        if len(x) &gt; 0:\n            subprocess.call(['hadoop', 'fs', '-mkdir', tmp_file_loc])\n\n        # save dataframe to tmp file \n        outfile = os.path.join(tmp_file_loc, \"temp_df\")\n        tmp_df.write.mode(\"overwrite\").parquet(outfile)\n        tmp_df = self.spark.read.parquet(outfile)\n        self.logger.info(\"data frame is written to %s to examine memory \\\n                         storage\", outfile)\n        df_size = self.__get_df_size(tmp_df)\n\n    self.logger.info(\"The dataframe takes an estimate of %s MB memory\", \"{:,.0f}\".format(df_size))\n\n    # get partition size . if partition size is too big or too small, adjust partition number \n\n    par_size = df_size / num_par\n    self.logger.info(\"Partition original size is n estimate of %d MB\", par_size) \n\n    if (par_size &lt;= 2 * self.partition_size * 2  ** 10) &amp; (par_size &gt;= self.partition_size * 2 ** 10 / 8):\n        # if partition size is moderate \n        new_par_num = self.total_exe_core\n        self.logger.info(\"Partition size is %d MB is moderate. Non need to change .\\n\"\n                         \"Partition number will be matched to the total executor core for higher perfomance\")\n    else:\n        if par_size &lt; self.partition_size * 2 ** 10 / 8:\n            self.logger.info(\"Partition size %d MB is too small. Repartition is recommended .\", par_size)\n        elif par_size &gt; 2 * self.partition_size * 2 ** 10:\n            self.logger.info(\"Partition size %d MB is too large, Repartition is recommended .\", par_size)\n\n        new_par_num = max(self.total_exe_core, \n                          min(\n                              rounding_ceil(df_size / self.partition_size / 2 ** 10, self.total_exe_core), 2 ** 30))\n\n    # tuning partition first    \n    if num_par &lt; new_par_num:\n        self.logger.warning(\"The input data partition %d is smaller then the tuned partition %d. \\n\"\n                            \"The data is being repartitioned for higher performance\", num_par, new_par_num)\n\n        spark_df = spark_df.coalesce(new_par_num)\n\n    else:\n        self.logger.info(\"The input data partition matches the tuned partition %d. \\n\"\n                         \"The data is able to archive best performance\", self.total_exe_core)\n\n    return spark_df\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.OptimizeSpark.tuning_read","title":"<code>tuning_read(filename, tuning=True, **kwargs)</code>","text":"<p>Read the file into Spark, and tune the dataframe by partition to match   total core number </p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>the full file path to be read from </p> required <code>tuning</code> <code>bool</code> <p>whether the dataframe needs to be automatically </p> <code>True</code> <code>**kwargs</code> <p>parameters from tune_dataframe</p> <code>{}</code> <p>Returns:     Spark Dataframe: the spark data frame read from spark, and tuned partition to match core numbers</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def tuning_read(self, filename, tuning=True, **kwargs):\n    \"\"\"\n    Read the file into Spark, and tune the dataframe by partition to match\n      total core number \n\n    Args:\n        filename (str): the full file path to be read from \n        tuning (bool): whether the dataframe needs to be automatically \n        tuned\n        **kwargs: parameters from tune_dataframe\n    Returns:\n        Spark Dataframe: the spark data frame read from spark, and tuned partition to match core numbers\n    \"\"\"\n    spark_df = self.spark.read.parquet(filename)\n    self.logger.info(\"Read Parquet File: %s\", filename)\n\n    if tuning:\n        spark_df = self.tune_dataframe(spark_df, **kwargs)\n    return spark_df\n</code></pre>"},{"location":"utils.html#src.op_spark.optimize_spark.rounding_ceil","title":"<code>rounding_ceil(x, n)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>_type_</code> required <code>n</code> <code>_type_</code> required <p>Returns:</p> Source code in <code>src/op_spark/optimize_spark.py</code> <pre><code>def rounding_ceil(x,n):\n    \"\"\"\n\n    Args:\n        x (_type_): \n        n (_type_):\n    Returns:\n\n    \"\"\"\n\n    if n == 1:\n        return ceil(x)\n    elif n &lt; 1:\n        logging.error(\"n should be positive integer\")\n    else:\n        return (ceil(x) + (n -1)) // n * n \n</code></pre>"}]}